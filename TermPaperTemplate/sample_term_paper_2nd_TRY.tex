%copyright ???
\documentclass[sigconf,  natbib, screen]{acmart}

% Documentation for packages
% - ACM Article Template
%    https://www.acm.org/publications/proceedings-template
% - Pseudocode typesetting CLRS-style:
%    https://www.cs.dartmouth.edu/~thc/clrscode/clrscode3e.pdf
% - Python code typesetting
%    http://ctan.uib.no/macros/latex/contrib/listings/listings.pdf
% - AMS Math
%    http://ctan.uib.no/macros/latex/required/amsmath/amsldoc.pdf
% - Graphics
%    http://ctan.uib.no/macros/latex/required/graphics/grfguide.pdf

\usepackage{verbatim}
\usepackage{clrscode3e}  
\usepackage{listings}
\lstset{language=Python, basicstyle=\ttfamily}
%for multiple columns
\usepackage{multicol}

%adding package for the font error
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% adding package to insert figures
\usepackage{graphicx}
% based on https://tex.stackexchange.com/questions/279240/float-for-lstlisting
\usepackage{float}
\floatstyle{ruled}
\newfloat{listing}{tbph}{lop}
\floatname{listing}{Listing}
\def\lstfloatautorefname{Listing} % needed for hyperref/auroref

\citestyle{acmauthoryear}

%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Benchmarking sorting algorithms in Python}
\subtitle{INF221 Term Paper, NMBU, Autumn 2019}

\author{Bishnu Poudel}
\email{bishnu.poudel@nmbu.no}
\affiliation{MS in Data Science, NMBU} 

\author{Mohamed Radwan}
\email{mohamed.radwan@nmbu.no}
\affiliation{MS in Data Science, NMBU} 

%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
In this paper, we generate the runtimes for three pure sorting algorithms that follow the divide-and-conquer approach. In addition, we benchmark the inbuilt numpy sort function- np.sort() and also the python sort function - sorted(). 
Then we compare these real-life runtimes with the theoritical runtimes that we have learnt as a part of the Algorithms 
and data structure course at NMBU. We also analyse and plot the distributions of runtimes we foundData is collected for list sizes  from 10 to 10 million in order to study the asymtotic behavior of these algorithms.
The work has been done using python and its libraries.
\end{abstract}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}\label{sec:intro}


Sorting and searching are one of the basic and routine operations for computers of any type. Much research has already been done in the subject, and computers today use the most robust algorithms possible that meet their specific requirement/application. Many applications use a blend of 2 or more algorithms depending on the nature input data or the application.  However, it would be interesting to see how the  sorting algorithms behave stand-alone  in real life.

In the following section, we are benchmarking sorting algorithms in python based on their runtimes. This paper is also the final 
term-paper for the 'Computer Science for Data Scientists'  course at NMBU. Here, we get to compare the theoritical runtimes of sorting algorithms  to their real-life runtimes.

The main questions we are trying to address are: Do the algorithms show their theoritical average case behavior in real situations? 
Which of the stand alone sort is more efficient? Do these standalone sorts have a chance against the built-in sorts of numpy and python?
How are the runtimes of each run distributed, is there any statistical significance in the distribution?

In the Theory section, we describe the psuedo-code of the algorithms together with their theoritical runtimes. We discuss the best, average and the worst cases.  In the Methods section, we describe the python implementation of the algorithms, together with the python functions  we 
wrote to extract the runtime information. We also discuss the type and amount of data we collected. Results section has facts and figures from 
our analysis. In the Discussion section, we summarize our findings and compare them to the theoritical expectations. We also compare the 
standalone algorithms with the pre-implemented algorithms in numpy and python. Acknowledgements and References conclude the paper.


\section{Theory}\label{sec:theory}
We briefly discuss the sorts we are benchmarking here. We wrote
python implementations for heap, merge and quick sort. Numpy's sort function and python's sorted function are used in their default form
without additional parameters.

\subsection{Heap Sort}\label{sec:heap sort}

\begin{listing}
  % Pseudocode caption above the code.
  \caption{Heap sort algorithm from \citet[Ch.~6.4]{CLRS_2009}.}
  \label{lst:heap_algo}

\begin{verbatim}
HEAP-SORT(A)
    BUILD-MAX_HEAP(A)
    for i = A.length downto 2
        swap A[1] and A[i]
        A.heap_size = A.heap_size - 1
        MAX-HEAPIFY(A, 1)
\end{verbatim}
\end{listing}

Heap sort uses the max-heap (or min heap property) to sort elements in an array. Due to this, heap sort is not stable. Therefore, the keys having exact value might be interchanged. Theoritically, all 3 cases of heap sort (worst, best and average)
are of the order $n*log(n)$. We wrote a python implementation of psuedo code above to sort the array in place. It will be interesting to see which of the three sets of data ( random, ordered, reversely ordered) data performs the best on heap sort. 

\subsection{Merge Sort}\label{sec:merge sort}

\begin{listing}
  % Pseudocode caption above the code.
  \caption{Merge sort algorithm from \citet[Ch.~2.3.1]{CLRS_2009}.}
  \label{lst:merge_algo}

\begin{verbatim}
MERGE-SORT(A,p,q)
    if p < r
    q = (p + r)/2
    MERGE-SORT(A, p, q)
    MERGE-SORT(A, q + 1, r)
    MERGE(A, p, q, r)
\end{verbatim}
\end{listing}

Here the MERGE function simply merges two lists, both of which are already sorted. Merge sort probably got its name from this sub-process of the algorithm.Merge sort is the most intuitive divide and conquer approach to sorting, as it is dividing the list 
recursively in two equal halves ( if the list has odd number of elements, one half has 1 more element than the other).

Merge sort has the runtime of the order of $n*log(n)$ in all of its best, average and worst cases. It will be interesting to see 
if the already sorted data is the best case for a merge sort.

\subsection{Quick Sort}\label{sec:quick sort}

\begin{listing}
  % Pseudocode caption above the code.
  \caption{Quick sort algorithm from \citet[Ch.~2.3.1]{CLRS_2009}.}
  \label{lst:quick_algo}

\begin{verbatim}
QUICKSORT(A, p, r)
    if p < r
        q = PARTITION(A, p, r)
        QUICKSORT(A, p, q-1)
        QUICKSORT(A, q+1, r)


PARTITION(A, p, r)
    x = A[r]
    i = p - 1
    for j = p to r - 1
        if A[j] <= x
            i = i + 1
            swap A[i] and A[j]
    swap A[i + 1] and A[r]
    return i + 1
\end{verbatim}
\end{listing}

In case of quicksort, most of the work is done by the PARTITION function. It does the inplace swapping of the list elements. It also 
returns the position of the pivot element to the QUICKSORT function.

 Quicksort has a average and best case runtime of the order of $n*log(n)$, (the best case has a smaller constant of course).
 However, in the case where each subsequent partition has only one less element than the last partition, it runs
into its worst case and has a theoritical runtime of $n^2$. It will be interesting to see if we indeed run into this scenario.

\subsection{Numpy's numpy.sort}\label{sec:numpy sort}
The numpy sort function uses quicksort and a combination of few other sorts depending on the input data. Yet to 
put reference article or link here...

\subsection{Python's default sorted}\label{sec:sorted sort}
Python uses a now popularly used sort algorithm in many programming languages, called timsort which is named after its
inventor. Yet to put reference article or link here...

\section{Methods}\label{sec:methods}

\subsection{Python implementation of algorithms}\label{sec:python implementation}

We wrote the python implementations for heap sort, merge sort and quick sort following the psuedo code from CLRS. We wrote
the code in a way that the only input we need to provide is the input array. The functions will calculate the length of the array 
or subarrayif their algorithm needs the length. Merge sort and quick sort call themselves recursively with 3 parameters, while heap
sort does it with a single parameter. All the python codes, data generated and figures can be found at https://github.com/vsnupoudel/termpaper01.
It was interesting to see that we could implement quicksort in a few lines. Below we can see the partition function used for quicksort.

\begin{listing}
  % Pseudocode caption above the code.
  \caption{Quick sort PARTITION FUNCTION}
  \label{lst:quick_partition}

\begin{verbatim}
def quick_comparison_and_swap (Inputlist, start, end): 
    pivot_last = Inputlist[end]
    index_less_than = start -1
    
    for comparison_index in range(start,end):
        if Inputlist[c_index]<= pivot_last:
            l_index += 1
            Inputlist[l_index], Inputlist[c_index]
            = Inputlist[c_index], Inputlist[l_index]  
            
    Inputlist[end] , Inputlist[l_index+1] 
    =   Inputlist[l_index+1], Inputlist[end]  

    return index_less_than+1  
\end{verbatim}
\end{listing}

\subsection{Timing function}\label{sec:timing function}

Secondly, to benchmark the 3 algorithms we  implemented along with numpy.sort() and sorted(), we use the timeit library. The skeleton of the 
function was provided by professor H.E. Plesser, which is built on. Since the process can slow down due to other processes in the computer, w
e repeated each of the timeit experiments 7 times and took the fastest time among the 7. We export the runtimes of these algorithms for 
list of size ranging from 10 to 10 million. We use the same seed all the time, so that the data we use is exactly same at all times.

\begin{listing}
  % Pseudocode caption above the code.
  \caption{Time it function used with parameters}
  \label{lst:time_it function}

\begin{verbatim}
import numpy as np
import timeit
import copy

def timing_function(number_of_data_points, sort_type
,randomization_type, seed_number=12235):
    np.random.seed (seed_number)
    test_data = np.random.random(number_of_data_points,)
    test_data = list(test_data)

    if randomization_type=='reverse':
        test_data= sorted(test_data, reverse =True)
    elif randomization_type=='sorted':
        test_data= sorted(test_data)

    clock= timeit.Timer(stmt='sort_func (copy(data))', 
                    globals ={ 'sort_func': sort_type,
                    'data': test_data ,
                    'copy': copy.copy })
    n_ar , t_ar = clock.autorange ()
    t = clock.repeat ( repeat =7, number = n_ar )
    return t
\end{verbatim}
\end{listing}

\subsection{Analysis, tables and plots}\label{sec:analysis}
After that, we compare the runtimes of 5 algorithms for all data sizes in a number of line graphs. Similary, we plot a scatter plot of the 7 runtimes 
we got for each of the algorithms and see if they show any statistical significance. We also compare the runtime for reverse sorted and already sorted data and see if it drastically different than the random data for any of the algorithm.

\subsection{Hardwares and softwares}\label{sec:hardwares and softwares}
The runtimes were obtained from a machine with the follwing configuration. 

Also, the following version of python and its libraries were used.
\begin{itemize}
\item Processor:    AMD A4-3330MX APU  2.30 GHz
\item Caches:        L1D-64 KB*2 , L1I-64KB*2, L2 -512KB*2
\item Memory :      DDR3 4 GB, 800 MHz
\item OS :              Microsoft Windows 7 Professional
\end{itemize}


Following are the versions of packages used in Python 3.7.4:
\begin{multicols}{2}
\begin{itemize}
\item Pandas:    '0.25.1'
\item Pandasql:    '0.7.3'
\item Numpy :      '1.16.5'
\item matplotlib :   '3.1.1'
\item nbimporter :  '0.3.1'
\item timeit :         
\item pickle :       
\item copy :        
\end{itemize}
\end{multicols}


\section{Results}\label{sec:results}

\subsection{All sorts for a particular permutation }\label{allsorts}

Figures \ref{fig:random1}-\ref{fig:random3} that follow present the runtime of all algorithms against the same randomized data.
We can see that numpy \textbf{sort} is the most efficient for large list size. Python's \textbf{sorted} function follows  numpy \textbf{sort} closely, until numpy-sort finally clinches victory around the 1 million mark.


\begin{figure}[ht]
\includegraphics[width=\linewidth]{"For -random from-10 to-1280".pdf}
    \caption{Randomized data of size 10-1280 }
    \label{fig:random1}
\end{figure}

In figure \ref{fig:random1}, it is worth noting that no clear winner can be judged from the graph. After around 20 thousand there, we can see which sort is faster. Also, note that the x axis of Figure \ref{fig:random1}  and Figure  \ref{fig:random2} are actually a $log base 2$ of the size of the input numeric array. While for figure 3, we have kept the original size on the x axis.

\begin{figure}[ht]
\includegraphics[width=\linewidth]{"For -random from-2560 to-40960".pdf}
    \caption{Randomized data of size 2560-40960}
    \label{fig:random2}
\end{figure}

Also, we have drawn graphs for $c1*log*n$ and $c2*log*n$  in the graph. We reached the values of c1 and c2 after a few trials and errors. As we can see, for smaller data sizes the run-times do not lie between our c1 an c2 lines. They do not show any linear behaviour either. In many cases larger data sizes are being sorted faster. This may also be due to other background processes running on our computer. 

\begin{figure}[ht]
\includegraphics[width=\linewidth]{"For -random from-81920 to-10485760".pdf}
    \caption{Randomized data of size 81920-10485760 }
    \label{fig:random3}
\end{figure}

Nevertheless, the conclusion that can be drawn from figure \ref{fig:random3} is that all of our algorithms show a asymptotic behavior of $nlog*n$. The only difference is that numpy sort and python sorted have extremely small constants compared to the others. We will describe this in detail in section \ref{math}.

%\begin{comment}
\begin{table}[ht]
\begin{center}

\begin{tabular}{|c|c|c|c|} 
\hline
Sort Type & List type & List length & runtime (sec) \\
\hline
numpy sort &	random &	10485760 &	0.632434 \\
python sorted &	random &	10485760 &	4.222582 \\
quick sort &	random &	10485760 &	32.919218 \\
merge sort &	random &	10485760 &	38.122927 \\
heap sort &	random &	10485760 &	96.435979 \\
\hline
\end{tabular}
\end{center}
\caption{runtimes for sorts for size 10485760}
\label{tab:table1}
\end{table}
%\end{comment}

Heap sort is the slowest asymptotically with the largest constant $c$. Also, we have to note that the value of the constant depends on the unit of time we choose. We have chosen microseconds here. Comparison is done mathematically in section \ref{math}.

\begin{figure}[ht]
\includegraphics[width=\linewidth]{"Numpy vs Python -sorted data 81920-10485760".pdf}
    \caption{Numpy vs Python with lower bound; array 81920-10485760 }
    \label{fig:numpyvspython}
\end{figure}


It is interesting compare the algorithms for sorted and reverse sorted data as well. We could not get the runtime for quick sort for reverse sorted and already sorted data, as it hits the maximum recursion error after the data size 2560. Figure \ref{fig:sorted1} shows the asymptotic nature of the algorithms for sorted data while Figure \ref{fig:rsorted1} shows it for reverse sorted data. Interestingly, for reverse sorted data the python sorted function is faster than numpy's sort function. Like the randomized data, the runtimes are asymtotically bounded between $c1*n*log(n)$  and $c2*n*log(n)$ , except the numpy sort in case of sorted data looks very close to the lower limit. On a closer look, numpy sort is indeed over the manually chosen limit of $0.00001$ , as shown in figure \ref{fig:numpyvspython}.

\begin{figure}[ht]
\includegraphics[width=\linewidth]{"For -sorted from-81920 to-10485760".pdf}
    \caption{Sorted data of size 81920-10485760 }
    \label{fig:sorted1}
\end{figure}

In figures \ref{fig:numpyvspython}, we have plotted numpy and python sorted function for alreaday sorted data, where the numpy sorted was very close to the lower bound in the previous figures. Here, we can see the separation when plotted separately.


\begin{figure}[ht]
\includegraphics[width=\linewidth]{"For -reverse_sorted from-81920 to-10485760".pdf}
    \caption{Reverse sorted data of size 81920-10485760 }
    \label{fig:rsorted1}
\end{figure}

\subsection{All permutations for a particular sort }\label{allpermut}

Next, we wanted to plot each sort in one graph with the 3 permutations: randomized, sorted and reverse sorted.

\subsubsection{Numpy sort}
Numpy sort is fastest with sorted data followed by random and reverse sorted data. The difference between sorted and reverse sorted data is significant as shown in the Figure \ref{fig:numpysort} and table below............. Table here.......................
\begin{figure}[ht]
\includegraphics[width=\linewidth]{"For -numpy_sort from-81920 to-10485760".pdf}
    \caption{Numpy sort for data of size 81920-10485760 }
    \label{fig:numpysort}
\end{figure}

\subsubsection{Python sorted}
Contrary to Numpy sort, python sorted is quite fast for reverse sorted data. If we look closely in Figure \ref{fig:rsorted1}, we cansee that python sorted was the ultimate winner among other sorts for reverse sorted data, which is a notable observation.
\begin{figure}[ht]
\includegraphics[width=\linewidth]{"For -python_sorted from-81920 to-10485760".pdf}
    \caption{Python sorted for data of size 81920-10485760 }
    \label{fig:pythonsorted}
\end{figure}

\subsubsection{Quick sort}
We don't have runtimes for reverse sorted and sorted data for quicksort yet, as we hit the maximum recursion limit, when we chose the last element as pivot. Figure \ref{fig:quicksort}
\begin{figure}[ht]
\includegraphics[width=\linewidth]{"For -quick_sort from-81920 to-10485760".pdf}
    \caption{Quick sort for data of size 81920-10485760 }
    \label{fig:quicksort}
\end{figure}

\subsubsection{Heap sort}
In case of heap sort, the sorted and reverse sorted data are doing better than random data as shown in Figure \ref{fig:heapsort} and also in table....
\begin{figure}[ht]
\includegraphics[width=\linewidth]{"For -heap_sort from-81920 to-10485760".pdf}
    \caption{Heap sort for data of size 81920-10485760 }
    \label{fig:heapsort}
\end{figure}

\subsubsection{Merge sort}
In case of merge sort, the times for all 3 permutations of data are giving mixed results. So the order seems to not matter .. Actual times for large array sizes are given below.
\begin{figure}[ht]
\includegraphics[width=\linewidth]{"For -merge_sort from-81920 to-10485760".pdf}
    \caption{Merge sort for data of size 81920-10485760 }
    \label{fig:mergesort}
\end{figure}


\subsection{Scatter plot of large data sizes }\label{scatter}
Further, we plotted the scatter plot of all the 7 time-points we had captured to check if we observe any anomalies or outliers. Up until now, we had considered only the minumum among the 7 in our analysis.

We have plotted here two scenarios in figure \ref{fig:scatterreverse} and   figure \ref{fig:scatterrandom}, where the runtimes had a large variation. There is a clear outlier in python sorted in figure \ref{fig:scatterreverse}. The variation in time, we believe is due to other applicatons or background processes running on our computer.
\begin{figure}[ht]
\includegraphics[width=\linewidth]{"Runtimes for datasize-10485760 in reverse_sorted".pdf}
    \caption{7 experiments for reverse sorted of size: 10485760 }
    \label{fig:scatterreverse}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=\linewidth]{"Runtimes for datasize-10485760 in random".pdf}
    \caption{7 experiments for random data of size: 10485760 }
    \label{fig:scatterrandom}
\end{figure}

\subsection{Ratio between heap sort and numpy sort}\label{math}
Here we try to compute the difference between the fastest sort and the slowest and give them appropriate equations. ............... Heap sort will get higher order logarithm than numpy sort.

\section{Discussion}\label{sec:discussion}

\begin{itemize}
\item Among the stand-alone sorts, quick sort was the quickest followed by merge sort and heap sort.
\item Numpy sort was fastest followed by python's sorted in case of random and sorted data, while python's sorted narrowly finished first in case of reverse sorted data
\item Runtimes for all the sorts do behave asymptotically $n*log(n)$. However, the default sort algorithms have quite small constants compared to the standalone algorithms. The ratio being $0.5/0.00001$ between heap sort and numpy sorted... Mathematical derivation in Results section...?????? 
\item Python sorted seems to have a soft corner for reverse sorted data.
\item Merge sort is insensitive to the initial sort order of data.
\item We have variation in considerable variance in runtimes due to other applications running in the computer. ( Actual variance value here) 
\item Heap sort has an order of Log of , while numpy sort has...........
\item Details on the project can be found at \href{https://github.com/vsnupoudel/termpaper01}{Github link here}
\end{itemize}



\section{Acknowledgements}\label{sec:acknowledgements}
We would like to thank Professor H.E Plesser and our TA Krista Gilman for their guidance and for answering our questions related to the paper.

%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample_bib}

\end{document}
