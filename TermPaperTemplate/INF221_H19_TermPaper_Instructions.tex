\documentclass[sigconf, nonacm, natbib, screen, balance=False]{acmart}

% Documentation for packages
% - ACM Article Template
%    https://www.acm.org/publications/proceedings-template
% - Pseudocode typesetting CLRS-style:
%    https://www.cs.dartmouth.edu/~thc/clrscode/clrscode3e.pdf
% - Python code typesetting
%    http://ctan.uib.no/macros/latex/contrib/listings/listings.pdf
% - AMS Math
%    http://ctan.uib.no/macros/latex/required/amsmath/amsldoc.pdf
% - Graphics
%    http://ctan.uib.no/macros/latex/required/graphics/grfguide.pdf

\usepackage{clrscode3e}  
\usepackage{listings}
\lstset{language=Python, basicstyle=\ttfamily}

% based on https://tex.stackexchange.com/questions/279240/float-for-lstlisting
\usepackage{float}
\floatstyle{ruled}
\newfloat{listing}{tbph}{lop}
\floatname{listing}{Listing}
\def\lstfloatautorefname{Listing} % needed for hyperref/auroref

\citestyle{acmauthoryear}


\begin{document}

\title{Benchmarking sorting algorithms in Python}
\subtitle{INF221 Term Paper, NMBU, Autumn 2019}

\author{Hans Ekkehard Plesser}
\affiliation{Data Science Section, Faculty of Science and Technology,
  Norwegain University of Life Sciences}
\email{hans.ekkehard.plesser@nmbu.no}

\begin{abstract}
Your work on the term paper shall train you in performing, analysing
and presenting benchmark experiments. As practical examples, sorting
algorithms are used and benchmarked against different types of
data. All algorithms are implemented in Python. As reference, also
Python's build-in sorting algorithms are benchmarked.
\end{abstract}



%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}\label{sec:intro}

In your term paper, you will perform benchmarks on a number of sorting
algorithms for test data of various size and structure, analyze your
results, visualize them graphically and present them together with
necessary theoretical background and discuss your results in relation
to expectations.

You shall collaborate in pairs as assigned for the regular exercises
in the course. Both students in a pair shall contribute reasonably
equally to all parts of the project, i.e., running benchmarks,
analyzing and visualizing results, and writing the paper. You shall
provide by
\begin{description}
\item[Friday, 15 Nov, 16.00] a complete draft of your term paper;
\item[Friday, 29 Nov, 16.00] the final version of your term paper.
\end{description}
You will receive feedback on your draft by 22 November.

These instructions will give information about the scope of this
project in Sec.~\ref{sec:scope}, the overall structure of the term
paper in Sec.~\ref{sec:structure}, provide instructions for
installation of necessary software in Sec.~\ref{sec:sw} followed by
technical suggestions for benchmarking and analysis in
Sec.~\ref{sec:benchanalyze}. Finally, Sec.~\ref{sec:tex} contains
information about writing a paper using \LaTeX.


\section{Scope}\label{sec:scope}

The goal of this term paper is to compare real-life behavior of
sorting algorithms with theoretical expectations.  This requires
\emph{measurements}: real-life, physical experiments on computer code
and data.  Important computer science issues you need to relate to in
your work on the paper are the correctness of code (actual
implementation, not a pseudo-code algorithm), since it makes no sense
to compare incorrect code; th suitable choice of test data, reflecting
relevant test cases and real-life situations; and the correctness of
the test setup, where a typical error would be that all but the first
of repeated tests run on sorted data.

In you paper, you shall implement and benchmark at least the following
sorting algorithms, using the algorithms presented in pseudocode in
the course:
\begin{itemize}
\item mergesort,
\item heapsort,
  \item quicksort.
\end{itemize}
In addition, you shall test the sort functions provide by Python and
by NumPy.

In your benchmarks, you shall use test data suitable to test the
behavior of the algorithms under worst case, best case and average
case scenarios. It is sufficient to test sorting of integers.

In order to study the scaling behavior of algorithms with problem
size, one usually increases problem size $n$ by a factor, e.g., $2$,
$10$ or $16$ instead of increasing problem size linearly.

You can limit your largest problem size so that the full set of all
benchmarks executes in no more than six hours on your computer.


\section{Paper structure and language}\label{sec:structure}

You paper shall follow the general structure of scientific papers in
experimental disciplines and have the following sections in this
order:
\begin{description}
  \item[Abstract] Give a concise, single-paragraph overview of the
    motivation for, goals of and results of the work presented. It
    should not contain references to the literature or figures in the
    main text.
  \item[Introduction] Provide a brief introduction into the topic and
    describe the purpose of the paper, the questions investigated and
    the key results obtained. Further, give an overview over the
    structure of the remainder of the paper.
  \item[Theory] Describe the algorithms you will benchmark and the
    expected run-times for the algorithms. Algorithms in pseudo-code
    will be useful in this part. In journal papers, this part is
    sometimes included in the introduction or methods section and kept
    very short, as experts in the field are expected to be familiar
    with the theory.
  \item[Methods] Describe how you implemented and tested the
    algorithms, how you generated test data and how you performed the
    benchmarks and analyzed results. Provide information on all
    hardware and software used, including version numbers, and
    describe where source code used and data generated can be
    found. You may include brief code excerpts in this part, but
    should not include the complete source code of algorithms tested.
  \item[Results] Present your results using figures and possibly also
    tables. Figures should be briefly described in figure captions and
    more thoroughly in the text. In this section, the focus should be
    on a factual description of the results, not on the
    interpretation.
  \item[Discussion] Summarize your findings, compare results obtained
    for different algorithms and different data, related them to
    expectations from theory, explain observations and place them in
    context.
  \item[Acknowledgements] Thank people who have helped you to prepare
    the paper, e.g., through technical advice or discussions. In
    journal papers, this part also includes information about funding
    sources.
  \item[References] Provide bibliographical information for all
    references cited in the paper. This section is usually
    automatically generated from your bibliography database using
    BibTeX or a similar reference management software.
  \end{description}
  
Most sections should be subdivided into subsections. Further levels of
hierarchy are possible, but be careful not too introduce too many
levels. A subdivision of a paper should usually have more that one
paragraph.

In a paper, you shall try to present a coherent story in a factual
style. It is not a diary of your journey from project start to
completed document, but rather an idealized report: Imagine that you
would do the work once again and take your reader through the process
from problem definition via data collection (methods) to results and
interpretation.

Some further suggestions:
\begin{itemize}
\item Write in active form, but use the impersonal
  scientific ``we''.
\item You can choose present or past tense, but be consistent.
\item Be precise, give actual values instead of terms such as
  ``large'' or ``tiny''.
\end{itemize}
Most importantly, make sure that you have (and show) the data to back
up any claims you make.

The NMBU Writing Centre provides resources for academic writing at \url{https://www.nmbu.no/en/students/writing/resources}.

\subsection{Plagiarism}\label{sec:plag}

Your term paper shall be \emph{your} work. You are encouraged to
consult sources from textbooks via scientific papers to online
material, but your must properly cite your sources. If you directly
copy material from other sources, you must mark it as a quote. For
long passages and for figures, you may need to secure the right to
include material by others in your texts. You are strongly encouraged
to work through the \emph{Write \& Cite} online course provided by
NMBU's Writing Centre at
\url{https://www.nmbu.no/en/students/writing/resources}.


\section{Software}\label{sec:sw}

\subsection{Software for benchmarking}\label{sec:swb}

Use Python for running all benchmarks. You can use your usual Python
setup, but it is good practice to create a dedicated Conda environment
for each research project to ensure that software versions remain
fixed during the course of a project, even if you move to newer
versions of some packages elsewhere. You can create a new environment
with
\begin{verbatim}
conda create --name inf221bench matplotlib scipy numpy \
   pandas jupyter jupyterlab scikit-learn spyder
\end{verbatim}


\subsection{Software for version control}\label{sec:swvc}
To keep a log of your benchmarking work including all source code,
results obtained and figures and text produced, you should register
all material related to this project in a Git source code repository.

You can install Git from \url{https://git-scm.com/downloads} for all
operating systems. On macOS, you can also install it via Homebrew and
under Linux using your packet manager. For sharing material between
project partners, you should set up a repository on Github, Bitbucket
or a similar platform.

A sample gitignore file covering auxiliary files automatically
generated by \LaTeX{} is included in the template archive.

\subsection{Software for writing}\label{sec:swtex}

For writing, you should use \LaTeX, as it is the most widely used tool
for writing professional-quality texts in math-heavy fields. The
following packages provide complete installation packages for
\LaTeX:

\begin{description}
\item[Windows] MikTeX from \url{https://miktex.org}
\item[macOS] MacTeX from \url{https://www.tug.org/mactex/}
\item[Linux] You should be able to install via your package manager
\end{description}
  
I would suggest to install the full packages under Windows or macOS,
even though they are rather large. If you are really pressed for disk
space, you can install just a minimal system, but then you may need to
install add-on packages later.

Overleaf\footnote{\url{https://www.overleaf.com}} has become a popular
platform for (co-)working on \LaTeX{} documents. NMBU has no
institutional subscription at present, but you can use a simple
version for free, and student discounts are available for the full service.

\section{Benchmarking and analysis}\label{sec:benchanalyze}

Your work should be split into two logical parts:
\begin{itemize}
\item generating benchmark data;
\item analyzing and visualizing benchmark data.
\end{itemize}

You can use Jupyter Notebooks or normal Python scripts for both
parts. The data generating step shall store benchmark results to disk,
where the analysis and visualisation step can collect this data. In
this way, you are flexible to optimize visualization without
interfering with data generation.

\subsection{Benchmarking}\label{sec:bench}

\subsubsection{Taking times}

\begin{listing}
\begin{lstlisting}
import numpy as np
import timeit
import copy

np.random.seed(12235)
test_data = np.random.random((1000,))

clock = timeit.Timer(stmt='sort_func(copy(data))',
         globals={'sort_func': sorted,
                  'data': test_data,
                  'copy': copy.copy})

n_ar, t_ar = clock.autorange()

t = clock.repeat(repeat=7, number=n_ar)
\end{lstlisting}
  \caption{Example script for timinig with \texttt{timeit.Timer}. See
    text for details.}
  \label{lst:timeit}
\end{listing}

We define some terminology first:
  \begin{itemize}
    \item
    Repetition:

    \begin{itemize}
        \item
      a single independent experiment
    \item
      we collect data from multiple repetitions to understand
      fluctuations in measurment process
    \end{itemize}
  \item
    Execution:

    \begin{itemize}
        \item
      a single call to function to be timed
    \item
      repeated many times to get sufficient interval for timing
    \item
      each execution must work on pristine data
    \end{itemize}
  \end{itemize}



Our measurement process then looks like this

  \begin{itemize}
    \item
    Single repetition

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
        \item
      Start timer
    \item
      Call timed function \(n\) times (\(n\)~executions)
    \item
      Stop timer
    \item
      Report time difference as result of repetition
    \end{enumerate}
  \item
    Selection of number of executions

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
        \item
      Start with some \(n\), e.g. \(n=1\) executions
    \item
      Perform single repetition
    \item
      If repetition took less than \(t_{\text{min}}\), set \(n=10n\) and
      go back to 2.
    \item
      We now have \(n_E\), the number of executions required per
      repetition (may depend on problem and problem size)
    \end{enumerate}
  \item
    Multiple repetitions

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
        \item
      Once a suitable number of executions has been found, perform \(r\)
      repetitions
    \item
      Collect results
    \item
      Report
    \end{enumerate}
\item
  Choosing \(t_{\text{min}}\) and \(r\)

  \begin{itemize}
    \item
    Python's \texttt{titmeit} uses by default \(t_{\text{min}}=0.2\)s,
    but for more reliable measurements 1s seems more appropriate
  \item
    \(r\) should be at least 3, but 5 or 7 does not hurt
  \end{itemize}
\end{itemize}




The most important Python tool for benchmarking is the Python
\verb!timeit!
module\footnote{\url{https://docs.python.org/3/library/timeit.html}},
which is part of standard Python. In order to have most control over
the benchmarking, use the \verb!Timer! class from this module.

Listing~\ref{lst:timeit} shows an example of how to do a single
benchmark with the \verb!Timer! class. Note the following points:
\begin{enumerate}
  \item We create a \verb!Timer! object called \verb!clock!, which we
    then use to run timing experiments.
  \item The \verb!stmt! argument is a string containing a Python
    statement. Execution of this statement will be timed. The
    statement applies \verb!sort_func! to a fresh copy of data.
  \item We need to provide a fresh copy of the data to be sorted on
    every call to the \verb!sort_func!; otherwise, when doing in-place
    sorts, only the first execution of the sorting function would sort
    the test data.
  \item The \verb!globals! dictionary passes variables from the scope
    of our script to the scope in which the \verb!stmt! is executed.
  \item \verb!clock.autorange()! figures out how many times
    \verb!stmt! needs to be executed so that total time taken is at
    least $0.2\,\text{ms}$; this number is returned as \verb!n_ar!.
  \item \verb!clock.repeat! performs \verb!repeat! repetitions of a
    benchmark experiment, executing \verb!stmt! \verb!number! times
    for each experiment. It returns a list \verb!repeat! execution
    times.
\end{enumerate}
You may want to take source code of the \verb!timeit! module to see
how it works.

\subsubsection{Managing code and results}

Benchmarks should be executed for different data sizes and differently
structured data as described in Sec.~\ref{sec:scope}. It may be a good
idea to split benchmarks across different scripts or notebooks, e.g., one script
per algorithm you test. In this way, you are more flexible and avoid a
single script that will run for a very long time.

Benchmark results should be collected in a Pandas dataframe in each
script and stored to file using \verb!to_pickle()!.  Ideally, your script
or notebook should write data to file automatically, no manual action
should be required. Choose filenames systematically.

To ensure maximum traceability of your results, you should commit all
source code to your repository before running a benchmark, and commit
the result files immediately after the benchmark is complete. Tools
such as Sumatra\footnote{\url{http://neuralensemble.org/sumatra/}}
\cite{Davi_2012_48} can automatize this, but you are not expected to
use them in this project.
  
\subsection{Analysis and visualization}\label{sec:analysis}

A Jupyter notebook is probably the most useful approach to analysing
and visualising benchmark results. The notebook should read the
pickled benchmark results from file, where useful combine data from
different benchmarks and in the end generate plots displaying
results. Choose figures that present the scaling properties of the
algorithms well and also indicate variations in measurement results.

Plots should be saved in PDF format for integration in the
final report document. For best results, figures should be created in
the size they will have in the final paper ($84\,\text{mm}$ wide for a
single column figure), with clear lettering in
not too small fonts (8 points or larger). A sample plotting script is
provided with material for this paper, showing how to set font and
figure sizes.

Choose colors wisely and
avoid clutter. In scientific articles one usually avoids legends in
graphs as well as figure titles on the top of figures. Line styles are
instead explained in the figure caption. Try to be consistent in use
of colors across figures and remember that many of us cannot
distinguish red from green.

If you want to include tables of results in your paper, remember that
Pandas can generate \LaTeX{} code for dataframes directly. To avoid
any copy-paste errors, you can write the resulting \LaTeX{} code
directly to a file and include that file automatically in the \LaTeX{}
file for your paper.


\section{Writing using \LaTeX}\label{sec:tex}

Use \LaTeX{} to write your paper. An archive containing a template for
your paper is available on the course website. The archive also
contains the source code for this document. By building on these example
\LaTeX{} source codes, you should be able to figure out how to write
code which \LaTeX{} then turns into a beautifully typeset document.
MiKTeX comes with the TeXworks frontend for LaTeX and MacTeX with
TeXShop. Those frontends should make running life reasonably easy.

Here are some links to introductions to working with \LaTeX{}:
\begin{itemize}
  \item \url{https://www.latex-tutorial.com}
\item \url{https://www.andy-roberts.net/writing/latex}
\item \url{https://www.dickimaw-books.com/latex/novices/}
\item \url{https://texfaq.org/FAQ-man-latex}
\end{itemize}


\subsection{Citations in \LaTeX}

\begin{table}
  \begin{tabular}{ll}
    \hline
    Command & Result \\\hline
    \verb!\cite{CLRS_2009}! & \cite{CLRS_2009} \\
    \verb!\citet{CLRS_2009}! & \citet{CLRS_2009} \\
    \verb!\cite[Ch.~2.1]{CLRS_2009}! & \cite[Ch.~2.1]{CLRS_2009} \\
    \verb!\citet[Ch.~2.1]{CLRS_2009}! & \citet[Ch.~2.1]{CLRS_2009} \\\hline
    \end{tabular}
  \caption{Citation commands in \LaTeX with the \texttt{natbib} package
  and the resulting citations in text.}\label{tab:cite}
\end{table}


To refer to a source in \LaTeX{} use the \verb!\cite! command and its
siblings as shown in Tab.~\ref{tab:cite}. The argument passed to the
\verb!\cite! command is the ``cite key'' for the work you want to
refer to, which is defined in the entry for that work in you
bibliography file. For the Cormen book, this entry looks like this

\begin{verbatim}
@book{CLRS_2009,
 author = {Cormen, Thomas H. and Leiserson, Charles E. 
           and Rivest, Ronald L. and Stein, Clifford},
 title = {Introduction to Algorithms, Third Edition},
 year = {2009},
 isbn = {0262033844, 9780262033848},
 edition = {3rd},
 publisher = {The MIT Press},
 address = {Cambridge, MA}
} 
\end{verbatim}

For more examples, see the \verb!sample_bib.bib! file included in the
template archive. You are welcome to use the file in your work. For
more on bibliographies in \LaTeX, see, e.g.,
\url{https://www.andy-roberts.net/writing/latex/bibliographies}.

If you do not run \LaTeX through TeXworks or TeXShop, you should run
the following sequence of commands to ensure that all references are
up to date:

\begin{verbatim}
pdflatex myfile
bibtex myfile
pdflatex myfile 
pdflatex myfile 
\end{verbatim}


%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample_bib}

\end{document}
